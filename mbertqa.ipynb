{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7045350,"sourceType":"datasetVersion","datasetId":3865755},{"sourceId":7131619,"sourceType":"datasetVersion","datasetId":4114666}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git lfs install\n!pip install transformers\n\n!GIT_LFS_SKIP_SMUDGE=1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-07T10:11:09.665246Z","iopub.execute_input":"2023-12-07T10:11:09.665675Z","iopub.status.idle":"2023-12-07T10:11:23.420055Z","shell.execute_reply.started":"2023-12-07T10:11:09.665643Z","shell.execute_reply":"2023-12-07T10:11:23.418709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\nimport json\nimport torch\nimport torch.nn as nn\nimport os\nfrom tqdm import tqdm\nfrom transformers import BertModel, BertTokenizerFast, AdamW\n# AutoTokenizer, AutoModelForQuestionAnswering, BertTokenizer, BertForQuestionAnswering\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import ExponentialLR\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:11:29.565400Z","iopub.execute_input":"2023-12-07T10:11:29.565877Z","iopub.status.idle":"2023-12-07T10:11:29.572274Z","shell.execute_reply.started":"2023-12-07T10:11:29.565840Z","shell.execute_reply":"2023-12-07T10:11:29.571213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_questions = 0","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:11:29.782576Z","iopub.execute_input":"2023-12-07T10:11:29.783349Z","iopub.status.idle":"2023-12-07T10:11:29.787712Z","shell.execute_reply.started":"2023-12-07T10:11:29.783313Z","shell.execute_reply":"2023-12-07T10:11:29.786643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data(questions , context , answers , spans):\n    \"\"\"\n    All are file paths , should read raw text from file \n    and convert into list\n    \"\"\"\n    \n    with open(questions) as f:\n        raw_questions = f.read().splitlines()\n        \n    with open(context) as f:\n        raw_context = f.read().splitlines()\n        \n    with open(answers) as f:\n        raw_answers = f.read().splitlines()\n        \n    with open(spans) as f:\n        raw_spans = f.read().splitlines()\n        \n    \n        \n    \"\"\"\n    answer = {'text' : ===== , 'answer_start': ======= , 'answer_end':=====}\n    \"\"\"\n    \n    Answers = []\n    for iter in range(len(raw_answers)):\n        dic = {}\n        dic['text'] = raw_answers[iter]\n\n#         start_index = raw_context[iter].index(dic['text'])\n#         end_index = start_index + len(dic['text'])\n#         dic['answer_start'] = start_index\n#         dic['answer_end'] = end_index\n            \n        Answers.append(dic)\n        \n    return raw_context, raw_questions, Answers       \n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:12:33.185654Z","iopub.execute_input":"2023-12-07T10:12:33.186470Z","iopub.status.idle":"2023-12-07T10:12:33.194482Z","shell.execute_reply.started":"2023-12-07T10:12:33.186430Z","shell.execute_reply":"2023-12-07T10:12:33.193315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_context, train_questions, train_answers = get_data(\"/kaggle/input/code-and-data/two_epochs/proj_dataset/train_data/real_que_tel.txt\",\n        \"/kaggle/input/code-and-data/two_epochs/proj_dataset/train_data/real_con_tel.txt\",\n        \"/kaggle/input/code-and-data/two_epochs/proj_dataset/train_data/real_ans_tel.txt\",\n        \"/kaggle/input/code-and-data/two_epochs/proj_dataset/train_data/real_span_tel.txt\")","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:12:33.439785Z","iopub.execute_input":"2023-12-07T10:12:33.440585Z","iopub.status.idle":"2023-12-07T10:12:35.060814Z","shell.execute_reply.started":"2023-12-07T10:12:33.440550Z","shell.execute_reply":"2023-12-07T10:12:35.059792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Context : {train_context[0]}\")\nprint(f\"Question : {train_questions[0]}\")\nprint(f\"Answer: {train_answers[0]}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:11:32.646285Z","iopub.status.idle":"2023-12-07T10:11:32.646603Z","shell.execute_reply.started":"2023-12-07T10:11:32.646446Z","shell.execute_reply":"2023-12-07T10:11:32.646460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_context, val_questions, val_answers = get_data(\"/kaggle/input/code-and-data/two_epochs/proj_dataset/test_data/real_que_tel.txt\",\n        \"/kaggle/input/code-and-data/two_epochs/proj_dataset/test_data/real_con_tel.txt\",\n        \"/kaggle/input/code-and-data/two_epochs/proj_dataset/test_data/real_ans_tel_c.txt\",\n        \"/kaggle/input/code-and-data/two_epochs/proj_dataset/test_data/real_span_tel.txt\")","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:12:47.394609Z","iopub.execute_input":"2023-12-07T10:12:47.395531Z","iopub.status.idle":"2023-12-07T10:12:47.473303Z","shell.execute_reply.started":"2023-12-07T10:12:47.395491Z","shell.execute_reply":"2023-12-07T10:12:47.472273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_lens = []\n\nfor txt in train_context:\n    txt = txt.strip()  # remove leading and trailing whitespaces\n    token_lens.append(len(txt.split(' ')))\n  \n\nprint(max(token_lens))\n\nplt.hist(token_lens,  bins=20)  # density=False would make counts\nplt.ylabel('Count')\nplt.xlabel('Length')\nplt.title('Distribution of Context Lengths');","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:12:49.420872Z","iopub.execute_input":"2023-12-07T10:12:49.421547Z","iopub.status.idle":"2023-12-07T10:12:50.730960Z","shell.execute_reply.started":"2023-12-07T10:12:49.421512Z","shell.execute_reply":"2023-12-07T10:12:50.730013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_lens2 = []\n\nfor txt in train_questions:\n    txt = txt.strip()  # remove leading and trailing whitespaces\n    token_lens2.append(len(txt.split(' ')))\n\n\nprint(max(token_lens2))\nprint(len(token_lens2))\n\nplt.hist(token_lens2,  bins=20)  # density=False would make counts\nplt.ylabel('Count')\nplt.xlabel('Length')\nplt.title('Distribution of Question Lengths');","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:12:50.732944Z","iopub.execute_input":"2023-12-07T10:12:50.733311Z","iopub.status.idle":"2023-12-07T10:12:51.484721Z","shell.execute_reply.started":"2023-12-07T10:12:50.733278Z","shell.execute_reply":"2023-12-07T10:12:51.483824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = 300 ","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:12:51.486001Z","iopub.execute_input":"2023-12-07T10:12:51.486365Z","iopub.status.idle":"2023-12-07T10:12:51.490709Z","shell.execute_reply.started":"2023-12-07T10:12:51.486330Z","shell.execute_reply":"2023-12-07T10:12:51.489720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForMaskedLM","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:12:51.492507Z","iopub.execute_input":"2023-12-07T10:12:51.492803Z","iopub.status.idle":"2023-12-07T10:12:51.505305Z","shell.execute_reply.started":"2023-12-07T10:12:51.492771Z","shell.execute_reply":"2023-12-07T10:12:51.504598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:12:51.506403Z","iopub.execute_input":"2023-12-07T10:12:51.506665Z","iopub.status.idle":"2023-12-07T10:12:53.206542Z","shell.execute_reply.started":"2023-12-07T10:12:51.506642Z","shell.execute_reply":"2023-12-07T10:12:53.205488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.is_fast","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:12:53.207722Z","iopub.execute_input":"2023-12-07T10:12:53.208041Z","iopub.status.idle":"2023-12-07T10:12:53.214074Z","shell.execute_reply.started":"2023-12-07T10:12:53.208015Z","shell.execute_reply":"2023-12-07T10:12:53.213171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_encodings = tokenizer(train_questions, train_context,  max_length = MAX_LENGTH, truncation=True, padding=True)\nvalid_encodings = tokenizer(val_questions, val_context,  max_length = MAX_LENGTH, truncation=True, padding = True)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:12:53.215341Z","iopub.execute_input":"2023-12-07T10:12:53.215640Z","iopub.status.idle":"2023-12-07T10:13:26.478386Z","shell.execute_reply.started":"2023-12-07T10:12:53.215615Z","shell.execute_reply":"2023-12-07T10:13:26.477533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(train_encodings)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:13:26.480808Z","iopub.execute_input":"2023-12-07T10:13:26.481124Z","iopub.status.idle":"2023-12-07T10:13:26.487239Z","shell.execute_reply.started":"2023-12-07T10:13:26.481099Z","shell.execute_reply":"2023-12-07T10:13:26.486131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_encodings.keys())\nprint(valid_encodings.keys())\nprint(len(train_encodings['input_ids']))\nprint(len(train_encodings['input_ids'][0]))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:13:26.488433Z","iopub.execute_input":"2023-12-07T10:13:26.488710Z","iopub.status.idle":"2023-12-07T10:13:26.499311Z","shell.execute_reply.started":"2023-12-07T10:13:26.488687Z","shell.execute_reply":"2023-12-07T10:13:26.498466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_encodings['input_ids'][1])","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:13:26.500317Z","iopub.execute_input":"2023-12-07T10:13:26.500662Z","iopub.status.idle":"2023-12-07T10:13:26.510815Z","shell.execute_reply.started":"2023-12-07T10:13:26.500638Z","shell.execute_reply":"2023-12-07T10:13:26.509890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ret_Answer_start_and_end_train(idx):\n    ret_start = 0\n    ret_end = 0\n    answer_encoding = tokenizer(train_answers[idx]['text'],  max_length = MAX_LENGTH, truncation=True, padding=True)\n    for a in range( len(train_encodings['input_ids'][idx]) -  len(answer_encoding['input_ids']) ): #len(train_encodings['input_ids'][0])):\n        match = True\n        iter = 0\n        for i in range(1,len(answer_encoding['input_ids']) - 1):\n            iter =i \n            if (answer_encoding['input_ids'][i] != train_encodings['input_ids'][idx][a + i]):\n                match = False\n                break\n        if match:\n            ret_start = a+1\n            ret_end = a+iter+1\n            break\n    return(ret_start, ret_end)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:15:19.234353Z","iopub.execute_input":"2023-12-07T10:15:19.234791Z","iopub.status.idle":"2023-12-07T10:15:19.242763Z","shell.execute_reply.started":"2023-12-07T10:15:19.234759Z","shell.execute_reply":"2023-12-07T10:15:19.241570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_rec=92\n\nz,x = ret_Answer_start_and_end_train(test_rec)\nprint(z, x)\n\npredict_answer_tokens = train_encodings.input_ids[test_rec][z : x]\nprint(tokenizer.decode(predict_answer_tokens))\nprint(train_answers[test_rec]['text'])\nprint(tokenizer.decode(train_encodings['input_ids'][test_rec]))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:15:20.791675Z","iopub.execute_input":"2023-12-07T10:15:20.792537Z","iopub.status.idle":"2023-12-07T10:15:20.804426Z","shell.execute_reply.started":"2023-12-07T10:15:20.792501Z","shell.execute_reply":"2023-12-07T10:15:20.803403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_encodings.keys())\nprint(valid_encodings.keys())\nprint(len(train_encodings['input_ids']))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:15:21.979634Z","iopub.execute_input":"2023-12-07T10:15:21.980545Z","iopub.status.idle":"2023-12-07T10:15:21.985714Z","shell.execute_reply.started":"2023-12-07T10:15:21.980508Z","shell.execute_reply":"2023-12-07T10:15:21.984785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_positions = []\nend_positions = []\nctr = 0\nfor h in range(len(train_encodings['input_ids'])):\n    #print(h)\n    s, e = ret_Answer_start_and_end_train(h)\n    start_positions.append(s)\n    end_positions.append(e)\n    if s==0:\n        ctr = ctr + 1\n\n    \ntrain_encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\nprint(ctr)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:15:23.169197Z","iopub.execute_input":"2023-12-07T10:15:23.169980Z","iopub.status.idle":"2023-12-07T10:15:52.688270Z","shell.execute_reply.started":"2023-12-07T10:15:23.169946Z","shell.execute_reply":"2023-12-07T10:15:52.687333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_encodings.keys())\nprint(valid_encodings.keys())\nprint(len(train_encodings['input_ids']))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:15:52.690077Z","iopub.execute_input":"2023-12-07T10:15:52.690386Z","iopub.status.idle":"2023-12-07T10:15:52.695687Z","shell.execute_reply.started":"2023-12-07T10:15:52.690353Z","shell.execute_reply":"2023-12-07T10:15:52.694700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_rec = 1\nprint(train_encodings['start_positions'][test_rec])\nprint(train_encodings['end_positions'][test_rec])\npredict_answer_tokens = train_encodings.input_ids[test_rec][train_encodings['start_positions'][test_rec] : train_encodings['end_positions'][test_rec]]\nprint(tokenizer.decode(predict_answer_tokens))\nprint(train_answers[test_rec]['text'])\nprint(tokenizer.decode(train_encodings['input_ids'][test_rec]))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:15:52.697067Z","iopub.execute_input":"2023-12-07T10:15:52.697414Z","iopub.status.idle":"2023-12-07T10:15:52.714781Z","shell.execute_reply.started":"2023-12-07T10:15:52.697382Z","shell.execute_reply":"2023-12-07T10:15:52.713736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ret_Answer_start_and_end_valid(idx):\n    ret_start = 0\n    ret_end = 0\n    answer_encoding = tokenizer(val_answers[idx]['text'],  max_length = MAX_LENGTH, truncation=True, padding=True)\n    for a in range( len(valid_encodings['input_ids'][idx])  -  len(answer_encoding['input_ids'])   ): #len(train_encodings_fast['input_ids'][0])):\n        match = True\n        for i in range(1,len(answer_encoding['input_ids']) - 1):\n            if (answer_encoding['input_ids'][i] != valid_encodings['input_ids'][idx][a + i]):\n                match = False\n                break\n        if match:\n            ret_start = a+1\n            ret_end = a+i+1\n            break\n    return(ret_start, ret_end)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:15:52.716818Z","iopub.execute_input":"2023-12-07T10:15:52.717162Z","iopub.status.idle":"2023-12-07T10:15:52.726467Z","shell.execute_reply.started":"2023-12-07T10:15:52.717136Z","shell.execute_reply":"2023-12-07T10:15:52.725560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_positions = []\nend_positions = []\nctr = 0\nfor h in range(len(valid_encodings['input_ids']) ):\n    #print(h)\n    s, e = ret_Answer_start_and_end_valid(h)\n    start_positions.append(s)\n    end_positions.append(e)\n    if s==0:\n        ctr = ctr + 1\n\n    \nvalid_encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\nprint(ctr)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:15:52.727540Z","iopub.execute_input":"2023-12-07T10:15:52.727845Z","iopub.status.idle":"2023-12-07T10:15:53.136990Z","shell.execute_reply.started":"2023-12-07T10:15:52.727820Z","shell.execute_reply":"2023-12-07T10:15:53.136132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_rec=2\n\nz,x = ret_Answer_start_and_end_valid(test_rec)\n\npredict_answer_tokens = valid_encodings.input_ids[test_rec][z : x]\nprint(tokenizer.decode(predict_answer_tokens))\nprint(val_answers[test_rec]['text'])\nprint(tokenizer.decode(valid_encodings['input_ids'][test_rec]))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:15:53.140327Z","iopub.execute_input":"2023-12-07T10:15:53.141212Z","iopub.status.idle":"2023-12-07T10:15:53.151589Z","shell.execute_reply.started":"2023-12-07T10:15:53.141172Z","shell.execute_reply":"2023-12-07T10:15:53.150627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_encodings.keys())\nprint(valid_encodings.keys())\nprint(len(train_encodings['input_ids']))\nprint(len(train_encodings['start_positions']))\nprint(len(train_encodings['end_positions']))\nprint(len(valid_encodings['input_ids']))\nprint(len(valid_encodings['start_positions']))\nprint(len(valid_encodings['end_positions']))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:16:02.230265Z","iopub.execute_input":"2023-12-07T10:16:02.230641Z","iopub.status.idle":"2023-12-07T10:16:02.237633Z","shell.execute_reply.started":"2023-12-07T10:16:02.230610Z","shell.execute_reply":"2023-12-07T10:16:02.236550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class InputDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n    def __getitem__(self, i):\n        return {\n            'input_ids': torch.tensor(self.encodings['input_ids'][i]),\n            'token_type_ids': torch.tensor(self.encodings['token_type_ids'][i]),\n            'attention_mask': torch.tensor(self.encodings['attention_mask'][i]),\n            'start_positions': torch.tensor(self.encodings['start_positions'][i]),\n            'end_positions': torch.tensor(self.encodings['end_positions'][i])\n        }\n    def __len__(self):\n        return len(self.encodings['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:16:05.340270Z","iopub.execute_input":"2023-12-07T10:16:05.341021Z","iopub.status.idle":"2023-12-07T10:16:05.347706Z","shell.execute_reply.started":"2023-12-07T10:16:05.340985Z","shell.execute_reply":"2023-12-07T10:16:05.346796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = InputDataset(train_encodings)\nvalid_dataset = InputDataset(valid_encodings)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:16:08.065266Z","iopub.execute_input":"2023-12-07T10:16:08.066078Z","iopub.status.idle":"2023-12-07T10:16:08.070449Z","shell.execute_reply.started":"2023-12-07T10:16:08.066045Z","shell.execute_reply":"2023-12-07T10:16:08.069359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nvalid_data_loader = DataLoader(valid_dataset, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:16:10.129181Z","iopub.execute_input":"2023-12-07T10:16:10.129939Z","iopub.status.idle":"2023-12-07T10:16:10.134638Z","shell.execute_reply.started":"2023-12-07T10:16:10.129904Z","shell.execute_reply":"2023-12-07T10:16:10.133579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone 'https://huggingface.co/bert-base-multilingual-cased'","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:16:16.989485Z","iopub.execute_input":"2023-12-07T10:16:16.989900Z","iopub.status.idle":"2023-12-07T10:16:40.219596Z","shell.execute_reply.started":"2023-12-07T10:16:16.989866Z","shell.execute_reply":"2023-12-07T10:16:40.218208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_PATH = '/kaggle/working/bert-base-multilingual-cased'","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:16:40.222339Z","iopub.execute_input":"2023-12-07T10:16:40.223227Z","iopub.status.idle":"2023-12-07T10:16:40.229984Z","shell.execute_reply.started":"2023-12-07T10:16:40.223194Z","shell.execute_reply":"2023-12-07T10:16:40.228127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model = BertModel.from_pretrained(MODEL_PATH)  #MODEL_PATH = \"bert-base-uncased\"\n\nclass QAModel(nn.Module):\n    def __init__(self):\n        super(QAModel, self).__init__()\n        self.bert = bert_model\n        self.drop_out = nn.Dropout(0.1)\n        self.l1 = nn.Linear(768 * 2, 768 * 2)\n        self.l2 = nn.Linear(768 * 2, 2)\n        self.linear_relu_stack = nn.Sequential(\n            self.drop_out,\n            self.l1,\n            nn.LeakyReLU(),\n            self.l2 \n        )\n        \n    def forward(self, input_ids, attention_mask, token_type_ids):\n        model_output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)\n        hidden_states = model_output[2]\n        out = torch.cat((hidden_states[-1], hidden_states[-3]), dim=-1)  # taking Start logits from last BERT layer, End Logits from third to last layer\n        logits = self.linear_relu_stack(out)\n        \n        start_logits, end_logits = logits.split(1, dim=-1)\n        \n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:16:40.232435Z","iopub.execute_input":"2023-12-07T10:16:40.232963Z","iopub.status.idle":"2023-12-07T10:16:42.923703Z","shell.execute_reply.started":"2023-12-07T10:16:40.232917Z","shell.execute_reply":"2023-12-07T10:16:42.922711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = QAModel()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:16:42.925604Z","iopub.execute_input":"2023-12-07T10:16:42.925915Z","iopub.status.idle":"2023-12-07T10:16:42.949842Z","shell.execute_reply.started":"2023-12-07T10:16:42.925890Z","shell.execute_reply":"2023-12-07T10:16:42.949117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:16:44.389439Z","iopub.execute_input":"2023-12-07T10:16:44.389832Z","iopub.status.idle":"2023-12-07T10:16:44.420034Z","shell.execute_reply.started":"2023-12-07T10:16:44.389802Z","shell.execute_reply":"2023-12-07T10:16:44.419011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    loss_fct = nn.CrossEntropyLoss()\n    start_loss = loss_fct(start_logits, start_positions)\n    end_loss = loss_fct(end_logits, end_positions)\n    total_loss = (start_loss + end_loss)/2\n    return total_loss","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:16:45.680636Z","iopub.execute_input":"2023-12-07T10:16:45.681611Z","iopub.status.idle":"2023-12-07T10:16:45.687219Z","shell.execute_reply.started":"2023-12-07T10:16:45.681571Z","shell.execute_reply":"2023-12-07T10:16:45.685686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def focal_loss_fn(start_logits, end_logits, start_positions, end_positions, gamma):\n    \n    #calculate Probabilities by applying Softmax to the Start and End Logits. Then get 1 - probabilities\n    smax = nn.Softmax(dim=1)\n    probs_start = smax(start_logits)\n    inv_probs_start = 1 - probs_start\n    probs_end = smax(end_logits)\n    inv_probs_end = 1 - probs_end\n    \n    #get log of probabilities. Note: NLLLoss required log probabilities. This is the Natural Log (Log base e)\n    lsmax = nn.LogSoftmax(dim=1)\n    log_probs_start = lsmax(start_logits)\n    log_probs_end = lsmax(end_logits)\n    \n    nll = nn.NLLLoss()\n    \n    fl_start = nll(torch.pow(inv_probs_start, gamma)* log_probs_start, start_positions)\n    fl_end = nll(torch.pow(inv_probs_end, gamma)*log_probs_end, end_positions)\n    \n    #return mean of the Loss for the start and end logits\n    return ((fl_start + fl_end)/2)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:16:46.964387Z","iopub.execute_input":"2023-12-07T10:16:46.965076Z","iopub.status.idle":"2023-12-07T10:16:46.971954Z","shell.execute_reply.started":"2023-12-07T10:16:46.965041Z","shell.execute_reply":"2023-12-07T10:16:46.970945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optim = AdamW(model.parameters(), lr=2e-5, weight_decay=2e-2)\nscheduler = ExponentialLR(optim, gamma=0.9)\ntotal_acc = []\ntotal_loss = []\n","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:16:48.664056Z","iopub.execute_input":"2023-12-07T10:16:48.664840Z","iopub.status.idle":"2023-12-07T10:16:48.676006Z","shell.execute_reply.started":"2023-12-07T10:16:48.664808Z","shell.execute_reply":"2023-12-07T10:16:48.675026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_epoch(model, dataloader, epoch):\n    model = model.train()\n    losses = []\n    acc = []\n    ctr = 0\n    batch_tracker = 0\n    for batch in tqdm(dataloader, desc = 'Running Epoch '):\n        optim.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n        out_start, out_end = model(input_ids=input_ids, \n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids)\n        #loss = loss_fn(out_start, out_end, start_positions, end_positions)  # <---BASELINE.  Cross Entropy Loss is returned by Default\n        loss = focal_loss_fn(out_start, out_end, start_positions, end_positions,1) #using gamma = 1\n        losses.append(loss.item())\n        loss.backward()\n        optim.step()\n        \n        start_pred = torch.argmax(out_start, dim=1)\n        end_pred = torch.argmax(out_end, dim=1)\n            \n        acc.append(((start_pred == start_positions).sum()/len(start_pred)).item())\n        acc.append(((end_pred == end_positions).sum()/len(end_pred)).item())\n        #ctr = ctr +1\n        #if ctr==50:\n        #    break\n        batch_tracker = batch_tracker + 1\n        if batch_tracker==250 and epoch==1:\n            total_acc.append(sum(acc)/len(acc))\n            loss_avg = sum(losses)/len(losses)\n            total_loss.append(loss_avg)\n            batch_tracker = 0\n    scheduler.step()\n    ret_acc = sum(acc)/len(acc)\n    ret_loss = sum(losses)/len(losses)\n    return(ret_acc, ret_loss)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:16:49.999453Z","iopub.execute_input":"2023-12-07T10:16:50.000411Z","iopub.status.idle":"2023-12-07T10:16:50.011815Z","shell.execute_reply.started":"2023-12-07T10:16:50.000366Z","shell.execute_reply":"2023-12-07T10:16:50.010761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, dataloader):\n    model = model.eval()\n    losses = []\n    acc = []\n    ctr = 0\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc = 'Running Evaluation'):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            start_true = batch['start_positions'].to(device)\n            end_true = batch['end_positions'].to(device)\n            \n            out_start, out_end = model(input_ids=input_ids, \n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids)\n            \n            start_pred = torch.argmax(out_start, dim=1)\n            end_pred = torch.argmax(out_end, dim=1)\n            \n            acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n            acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n            #ctr = ctr +1\n            #if ctr==50:\n            #    break\n        ret_acc = sum(acc)/len(acc)\n        ret_loss = 0\n        #ret_loss = sum(losses)/len(losses)\n    return(ret_acc)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:16:51.554295Z","iopub.execute_input":"2023-12-07T10:16:51.555043Z","iopub.status.idle":"2023-12-07T10:16:51.690646Z","shell.execute_reply.started":"2023-12-07T10:16:51.555006Z","shell.execute_reply":"2023-12-07T10:16:51.689769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 5\n\nmodel.to(device)\n\nfor epoch in range(EPOCHS):\n    train_acc, train_loss = train_epoch(model, train_data_loader, epoch+1)\n    print(f\"Train Accuracy: {train_acc}      Train Loss: {train_loss}\")\n    val_acc = eval_model(model, valid_data_loader)\n    print(f\"Validation Accuracy: {val_acc}\")\n\ntorch.save(model.state_dict(), \"QA_finutunemodel.pt\")","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:16:52.830212Z","iopub.execute_input":"2023-12-07T10:16:52.830604Z","iopub.status.idle":"2023-12-07T10:17:03.622073Z","shell.execute_reply.started":"2023-12-07T10:16:52.830574Z","shell.execute_reply":"2023-12-07T10:17:03.620779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_answer(question, context):\n    inputs = tokenizerFast.encode_plus(question, context, return_tensors='pt').to(device)\n    with torch.no_grad():\n        output_start, output_end = model(**inputs)\n        \n        answer_start = torch.argmax(output_start)  \n        answer_end = torch.argmax(output_end) \n\n        answer = tokenizerFast.convert_tokens_to_string(tokenizerFast.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n\n        return(answer)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T07:06:32.578919Z","iopub.execute_input":"2023-12-07T07:06:32.579766Z","iopub.status.idle":"2023-12-07T07:06:32.587220Z","shell.execute_reply.started":"2023-12-07T07:06:32.579722Z","shell.execute_reply":"2023-12-07T07:06:32.585977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Finding start and end span indices\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}