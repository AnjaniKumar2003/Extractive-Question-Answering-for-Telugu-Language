{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7045350,"sourceType":"datasetVersion","datasetId":3865755},{"sourceId":7131619,"sourceType":"datasetVersion","datasetId":4114666}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git lfs install\n!pip install transformers\n\n!GIT_LFS_SKIP_SMUDGE=1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-07T11:10:13.668254Z","iopub.execute_input":"2023-12-07T11:10:13.668850Z","iopub.status.idle":"2023-12-07T11:10:28.392811Z","shell.execute_reply.started":"2023-12-07T11:10:13.668820Z","shell.execute_reply":"2023-12-07T11:10:28.391485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\nimport json\nimport torch\nimport torch.nn as nn\nimport os\nfrom tqdm import tqdm\nfrom transformers import BertModel, BertTokenizerFast, AdamW\n# AutoTokenizer, AutoModelForQuestionAnswering, BertTokenizer, BertForQuestionAnswering\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import ExponentialLR\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:10:28.394717Z","iopub.execute_input":"2023-12-07T11:10:28.395106Z","iopub.status.idle":"2023-12-07T11:10:33.366457Z","shell.execute_reply.started":"2023-12-07T11:10:28.395072Z","shell.execute_reply":"2023-12-07T11:10:33.365525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_questions = 0","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:10:33.368587Z","iopub.execute_input":"2023-12-07T11:10:33.369487Z","iopub.status.idle":"2023-12-07T11:10:33.374333Z","shell.execute_reply.started":"2023-12-07T11:10:33.369447Z","shell.execute_reply":"2023-12-07T11:10:33.373372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data(questions , context , answers , spans):\n    \"\"\"\n    All are file paths , should read raw text from file \n    and convert into list\n    \"\"\"\n    \n    with open(questions) as f:\n        raw_questions = f.read().splitlines()\n        \n    with open(context) as f:\n        raw_context = f.read().splitlines()\n        \n    with open(answers) as f:\n        raw_answers = f.read().splitlines()\n        \n    with open(spans) as f:\n        raw_spans = f.read().splitlines()\n        \n    \n        \n    \"\"\"\n    answer = {'text' : ===== , 'answer_start': ======= , 'answer_end':=====}\n    \"\"\"\n    \n    Answers = []\n    for iter in range(len(raw_answers)):\n        dic = {}\n        dic['text'] = raw_answers[iter]\n\n#         start_index = raw_context[iter].index(dic['text'])\n#         end_index = start_index + len(dic['text'])\n#         dic['answer_start'] = start_index\n#         dic['answer_end'] = end_index\n            \n        Answers.append(dic)\n        \n    return raw_context, raw_questions, Answers       \n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:10:33.377104Z","iopub.execute_input":"2023-12-07T11:10:33.377506Z","iopub.status.idle":"2023-12-07T11:10:33.387555Z","shell.execute_reply.started":"2023-12-07T11:10:33.377475Z","shell.execute_reply":"2023-12-07T11:10:33.386781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_context, train_questions, train_answers = get_data(\"/kaggle/input/code-and-data/two_epochs/proj_dataset/train_data/real_que_tel.txt\",\n        \"/kaggle/input/code-and-data/two_epochs/proj_dataset/train_data/real_con_tel.txt\",\n        \"/kaggle/input/code-and-data/two_epochs/proj_dataset/train_data/real_ans_tel.txt\",\n        \"/kaggle/input/code-and-data/two_epochs/proj_dataset/train_data/real_span_tel.txt\")","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:10:33.389002Z","iopub.execute_input":"2023-12-07T11:10:33.389358Z","iopub.status.idle":"2023-12-07T11:10:36.485209Z","shell.execute_reply.started":"2023-12-07T11:10:33.389326Z","shell.execute_reply":"2023-12-07T11:10:36.484415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Context : {train_context[0]}\")\nprint(f\"Question : {train_questions[0]}\")\nprint(f\"Answer: {train_answers[0]}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:10:36.486313Z","iopub.execute_input":"2023-12-07T11:10:36.486608Z","iopub.status.idle":"2023-12-07T11:10:36.491516Z","shell.execute_reply.started":"2023-12-07T11:10:36.486582Z","shell.execute_reply":"2023-12-07T11:10:36.490588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_context, val_questions, val_answers = get_data(\"/kaggle/input/code-and-data/two_epochs/proj_dataset/test_data/real_que_tel.txt\",\n        \"/kaggle/input/code-and-data/two_epochs/proj_dataset/test_data/real_con_tel.txt\",\n        \"/kaggle/input/code-and-data/two_epochs/proj_dataset/test_data/real_ans_tel_c.txt\",\n        \"/kaggle/input/code-and-data/two_epochs/proj_dataset/test_data/real_span_tel.txt\")","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:10:36.493170Z","iopub.execute_input":"2023-12-07T11:10:36.493558Z","iopub.status.idle":"2023-12-07T11:10:36.555938Z","shell.execute_reply.started":"2023-12-07T11:10:36.493526Z","shell.execute_reply":"2023-12-07T11:10:36.555230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_lens = []\n\nfor txt in train_context:\n    txt = txt.strip()  # remove leading and trailing whitespaces\n    token_lens.append(len(txt.split(' ')))\n  \n\nprint(max(token_lens))\n\nplt.hist(token_lens,  bins=20)  # density=False would make counts\nplt.ylabel('Count')\nplt.xlabel('Length')\nplt.title('Distribution of Context Lengths');","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:10:36.556840Z","iopub.execute_input":"2023-12-07T11:10:36.557079Z","iopub.status.idle":"2023-12-07T11:10:37.875707Z","shell.execute_reply.started":"2023-12-07T11:10:36.557058Z","shell.execute_reply":"2023-12-07T11:10:37.874803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_lens2 = []\n\nfor txt in train_questions:\n    txt = txt.strip()  # remove leading and trailing whitespaces\n    token_lens2.append(len(txt.split(' ')))\n\n\nprint(max(token_lens2))\nprint(len(token_lens2))\n\nplt.hist(token_lens2,  bins=20)  # density=False would make counts\nplt.ylabel('Count')\nplt.xlabel('Length')\nplt.title('Distribution of Question Lengths');","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:10:37.876958Z","iopub.execute_input":"2023-12-07T11:10:37.877326Z","iopub.status.idle":"2023-12-07T11:10:38.632921Z","shell.execute_reply.started":"2023-12-07T11:10:37.877284Z","shell.execute_reply":"2023-12-07T11:10:38.631982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = 300 ","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:10:55.824066Z","iopub.execute_input":"2023-12-07T11:10:55.825103Z","iopub.status.idle":"2023-12-07T11:10:55.829101Z","shell.execute_reply.started":"2023-12-07T11:10:55.825064Z","shell.execute_reply":"2023-12-07T11:10:55.828165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:10:44.248305Z","iopub.execute_input":"2023-12-07T11:10:44.249061Z","iopub.status.idle":"2023-12-07T11:10:48.662822Z","shell.execute_reply.started":"2023-12-07T11:10:44.249022Z","shell.execute_reply":"2023-12-07T11:10:48.661869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-12-07T10:12:51.506403Z","iopub.execute_input":"2023-12-07T10:12:51.506665Z","iopub.status.idle":"2023-12-07T10:12:53.206542Z","shell.execute_reply.started":"2023-12-07T10:12:51.506642Z","shell.execute_reply":"2023-12-07T10:12:53.205488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.is_fast","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:11:01.188203Z","iopub.execute_input":"2023-12-07T11:11:01.188537Z","iopub.status.idle":"2023-12-07T11:11:01.194613Z","shell.execute_reply.started":"2023-12-07T11:11:01.188511Z","shell.execute_reply":"2023-12-07T11:11:01.193706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_encodings = tokenizer(train_questions, train_context,  max_length = MAX_LENGTH, truncation=True, padding=True)\nvalid_encodings = tokenizer(val_questions, val_context,  max_length = MAX_LENGTH, truncation=True, padding = True)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:11:02.048562Z","iopub.execute_input":"2023-12-07T11:11:02.049466Z","iopub.status.idle":"2023-12-07T11:11:34.799910Z","shell.execute_reply.started":"2023-12-07T11:11:02.049429Z","shell.execute_reply":"2023-12-07T11:11:34.799111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(train_encodings)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:11:34.801823Z","iopub.execute_input":"2023-12-07T11:11:34.802509Z","iopub.status.idle":"2023-12-07T11:11:34.808304Z","shell.execute_reply.started":"2023-12-07T11:11:34.802473Z","shell.execute_reply":"2023-12-07T11:11:34.807403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_encodings.keys())\nprint(valid_encodings.keys())\nprint(len(train_encodings['input_ids']))\nprint(len(train_encodings['input_ids'][0]))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:11:34.809329Z","iopub.execute_input":"2023-12-07T11:11:34.809589Z","iopub.status.idle":"2023-12-07T11:11:34.820626Z","shell.execute_reply.started":"2023-12-07T11:11:34.809566Z","shell.execute_reply":"2023-12-07T11:11:34.819796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_encodings['input_ids'][1])","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:11:34.823036Z","iopub.execute_input":"2023-12-07T11:11:34.823613Z","iopub.status.idle":"2023-12-07T11:11:34.832683Z","shell.execute_reply.started":"2023-12-07T11:11:34.823580Z","shell.execute_reply":"2023-12-07T11:11:34.831665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ret_Answer_start_and_end_train(idx):\n    ret_start = 0\n    ret_end = 0\n    answer_encoding = tokenizer(train_answers[idx]['text'],  max_length = MAX_LENGTH, truncation=True, padding=True)\n    for a in range( len(train_encodings['input_ids'][idx]) -  len(answer_encoding['input_ids']) ): #len(train_encodings['input_ids'][0])):\n        match = True\n        iter = 0\n        for i in range(1,len(answer_encoding['input_ids']) - 1):\n            iter =i \n            if (answer_encoding['input_ids'][i] != train_encodings['input_ids'][idx][a + i]):\n                match = False\n                break\n        if match:\n            ret_start = a+1\n            ret_end = a+iter+1\n            break\n    return(ret_start, ret_end)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:11:34.833898Z","iopub.execute_input":"2023-12-07T11:11:34.834181Z","iopub.status.idle":"2023-12-07T11:11:34.843004Z","shell.execute_reply.started":"2023-12-07T11:11:34.834158Z","shell.execute_reply":"2023-12-07T11:11:34.842159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_rec=92\n\nz,x = ret_Answer_start_and_end_train(test_rec)\nprint(z, x)\n\npredict_answer_tokens = train_encodings.input_ids[test_rec][z : x]\nprint(tokenizer.decode(predict_answer_tokens))\nprint(train_answers[test_rec]['text'])\nprint(tokenizer.decode(train_encodings['input_ids'][test_rec]))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:11:34.846255Z","iopub.execute_input":"2023-12-07T11:11:34.846506Z","iopub.status.idle":"2023-12-07T11:11:45.183316Z","shell.execute_reply.started":"2023-12-07T11:11:34.846485Z","shell.execute_reply":"2023-12-07T11:11:45.182340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_encodings.keys())\nprint(valid_encodings.keys())\nprint(len(train_encodings['input_ids']))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:11:45.184388Z","iopub.execute_input":"2023-12-07T11:11:45.184931Z","iopub.status.idle":"2023-12-07T11:11:45.190202Z","shell.execute_reply.started":"2023-12-07T11:11:45.184902Z","shell.execute_reply":"2023-12-07T11:11:45.189319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_positions = []\nend_positions = []\nctr = 0\nfor h in range(len(train_encodings['input_ids'])):\n    #print(h)\n    s, e = ret_Answer_start_and_end_train(h)\n    start_positions.append(s)\n    end_positions.append(e)\n    if s==0:\n        ctr = ctr + 1\n\n    \ntrain_encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\nprint(ctr)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:11:45.191844Z","iopub.execute_input":"2023-12-07T11:11:45.192459Z","iopub.status.idle":"2023-12-07T11:12:21.706722Z","shell.execute_reply.started":"2023-12-07T11:11:45.192425Z","shell.execute_reply":"2023-12-07T11:12:21.705737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_encodings.keys())\nprint(valid_encodings.keys())\nprint(len(train_encodings['input_ids']))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:12:21.708071Z","iopub.execute_input":"2023-12-07T11:12:21.708409Z","iopub.status.idle":"2023-12-07T11:12:21.713585Z","shell.execute_reply.started":"2023-12-07T11:12:21.708382Z","shell.execute_reply":"2023-12-07T11:12:21.712712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_rec = 1\nprint(train_encodings['start_positions'][test_rec])\nprint(train_encodings['end_positions'][test_rec])\npredict_answer_tokens = train_encodings.input_ids[test_rec][train_encodings['start_positions'][test_rec] : train_encodings['end_positions'][test_rec]]\nprint(tokenizer.decode(predict_answer_tokens))\nprint(train_answers[test_rec]['text'])\nprint(tokenizer.decode(train_encodings['input_ids'][test_rec]))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:12:21.716559Z","iopub.execute_input":"2023-12-07T11:12:21.716860Z","iopub.status.idle":"2023-12-07T11:12:21.729654Z","shell.execute_reply.started":"2023-12-07T11:12:21.716836Z","shell.execute_reply":"2023-12-07T11:12:21.728798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ret_Answer_start_and_end_valid(idx):\n    ret_start = 0\n    ret_end = 0\n    answer_encoding = tokenizer(val_answers[idx]['text'],  max_length = MAX_LENGTH, truncation=True, padding=True)\n    for a in range( len(valid_encodings['input_ids'][idx])  -  len(answer_encoding['input_ids'])   ): #len(train_encodings_fast['input_ids'][0])):\n        match = True\n        for i in range(1,len(answer_encoding['input_ids']) - 1):\n            if (answer_encoding['input_ids'][i] != valid_encodings['input_ids'][idx][a + i]):\n                match = False\n                break\n        if match:\n            ret_start = a+1\n            ret_end = a+i+1\n            break\n    return(ret_start, ret_end)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:12:21.730625Z","iopub.execute_input":"2023-12-07T11:12:21.730895Z","iopub.status.idle":"2023-12-07T11:12:21.740987Z","shell.execute_reply.started":"2023-12-07T11:12:21.730872Z","shell.execute_reply":"2023-12-07T11:12:21.740198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_positions = []\nend_positions = []\nctr = 0\nfor h in range(len(valid_encodings['input_ids']) ):\n    #print(h)\n    s, e = ret_Answer_start_and_end_valid(h)\n    start_positions.append(s)\n    end_positions.append(e)\n    if s==0:\n        ctr = ctr + 1\n\n    \nvalid_encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\nprint(ctr)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:12:21.741969Z","iopub.execute_input":"2023-12-07T11:12:21.742259Z","iopub.status.idle":"2023-12-07T11:12:22.223984Z","shell.execute_reply.started":"2023-12-07T11:12:21.742236Z","shell.execute_reply":"2023-12-07T11:12:22.222909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_rec=2\n\nz,x = ret_Answer_start_and_end_valid(test_rec)\n\npredict_answer_tokens = valid_encodings.input_ids[test_rec][z : x]\nprint(tokenizer.decode(predict_answer_tokens))\nprint(val_answers[test_rec]['text'])\nprint(tokenizer.decode(valid_encodings['input_ids'][test_rec]))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:12:22.225816Z","iopub.execute_input":"2023-12-07T11:12:22.226165Z","iopub.status.idle":"2023-12-07T11:12:22.240661Z","shell.execute_reply.started":"2023-12-07T11:12:22.226132Z","shell.execute_reply":"2023-12-07T11:12:22.239797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_encodings.keys())\nprint(valid_encodings.keys())\nprint(len(train_encodings['input_ids']))\nprint(len(train_encodings['start_positions']))\nprint(len(train_encodings['end_positions']))\nprint(len(valid_encodings['input_ids']))\nprint(len(valid_encodings['start_positions']))\nprint(len(valid_encodings['end_positions']))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:12:22.242011Z","iopub.execute_input":"2023-12-07T11:12:22.242656Z","iopub.status.idle":"2023-12-07T11:12:22.249848Z","shell.execute_reply.started":"2023-12-07T11:12:22.242630Z","shell.execute_reply":"2023-12-07T11:12:22.248958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class InputDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n    def __getitem__(self, i):\n        return {\n            'input_ids': torch.tensor(self.encodings['input_ids'][i]),\n            'token_type_ids': torch.tensor(self.encodings['token_type_ids'][i]),\n            'attention_mask': torch.tensor(self.encodings['attention_mask'][i]),\n            'start_positions': torch.tensor(self.encodings['start_positions'][i]),\n            'end_positions': torch.tensor(self.encodings['end_positions'][i])\n        }\n    def __len__(self):\n        return len(self.encodings['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:12:22.251044Z","iopub.execute_input":"2023-12-07T11:12:22.251355Z","iopub.status.idle":"2023-12-07T11:12:22.259614Z","shell.execute_reply.started":"2023-12-07T11:12:22.251329Z","shell.execute_reply":"2023-12-07T11:12:22.258721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = InputDataset(train_encodings)\nvalid_dataset = InputDataset(valid_encodings)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:12:22.260953Z","iopub.execute_input":"2023-12-07T11:12:22.261244Z","iopub.status.idle":"2023-12-07T11:12:22.271854Z","shell.execute_reply.started":"2023-12-07T11:12:22.261220Z","shell.execute_reply":"2023-12-07T11:12:22.271026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nvalid_data_loader = DataLoader(valid_dataset, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:12:22.272837Z","iopub.execute_input":"2023-12-07T11:12:22.275672Z","iopub.status.idle":"2023-12-07T11:12:22.282885Z","shell.execute_reply.started":"2023-12-07T11:12:22.275639Z","shell.execute_reply":"2023-12-07T11:12:22.282078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone 'https://huggingface.co/ai4bharat/indic-bert'","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:13:13.678034Z","iopub.execute_input":"2023-12-07T11:13:13.678497Z","iopub.status.idle":"2023-12-07T11:13:17.186639Z","shell.execute_reply.started":"2023-12-07T11:13:13.678455Z","shell.execute_reply":"2023-12-07T11:13:17.185438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_PATH = '/kaggle/working/indic-bert'","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:13:58.957950Z","iopub.execute_input":"2023-12-07T11:13:58.958360Z","iopub.status.idle":"2023-12-07T11:13:58.963146Z","shell.execute_reply.started":"2023-12-07T11:13:58.958327Z","shell.execute_reply":"2023-12-07T11:13:58.962202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model = BertModel.from_pretrained(MODEL_PATH)  #MODEL_PATH = \"bert-base-uncased\"\n\nclass QAModel(nn.Module):\n    def __init__(self):\n        super(QAModel, self).__init__()\n        self.bert = bert_model\n        self.drop_out = nn.Dropout(0.1)\n        self.l1 = nn.Linear(768 * 2, 768 * 2)\n        self.l2 = nn.Linear(768 * 2, 2)\n        self.linear_relu_stack = nn.Sequential(\n            self.drop_out,\n            self.l1,\n            nn.LeakyReLU(),\n            self.l2 \n        )\n        \n    def forward(self, input_ids, attention_mask, token_type_ids):\n        model_output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)\n        hidden_states = model_output[2]\n        out = torch.cat((hidden_states[-1], hidden_states[-3]), dim=-1)  # taking Start logits from last BERT layer, End Logits from third to last layer\n        logits = self.linear_relu_stack(out)\n        \n        start_logits, end_logits = logits.split(1, dim=-1)\n        \n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:14:00.788945Z","iopub.execute_input":"2023-12-07T11:14:00.789369Z","iopub.status.idle":"2023-12-07T11:14:04.637429Z","shell.execute_reply.started":"2023-12-07T11:14:00.789333Z","shell.execute_reply":"2023-12-07T11:14:04.636349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = QAModel()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:14:10.454099Z","iopub.execute_input":"2023-12-07T11:14:10.454479Z","iopub.status.idle":"2023-12-07T11:14:10.475026Z","shell.execute_reply.started":"2023-12-07T11:14:10.454448Z","shell.execute_reply":"2023-12-07T11:14:10.474062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:14:11.458230Z","iopub.execute_input":"2023-12-07T11:14:11.458971Z","iopub.status.idle":"2023-12-07T11:14:11.486975Z","shell.execute_reply.started":"2023-12-07T11:14:11.458935Z","shell.execute_reply":"2023-12-07T11:14:11.486000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    loss_fct = nn.CrossEntropyLoss()\n    start_loss = loss_fct(start_logits, start_positions)\n    end_loss = loss_fct(end_logits, end_positions)\n    total_loss = (start_loss + end_loss)/2\n    return total_loss","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:14:12.618655Z","iopub.execute_input":"2023-12-07T11:14:12.619017Z","iopub.status.idle":"2023-12-07T11:14:12.624012Z","shell.execute_reply.started":"2023-12-07T11:14:12.618989Z","shell.execute_reply":"2023-12-07T11:14:12.623091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def focal_loss_fn(start_logits, end_logits, start_positions, end_positions, gamma):\n    \n    #calculate Probabilities by applying Softmax to the Start and End Logits. Then get 1 - probabilities\n    smax = nn.Softmax(dim=1)\n    probs_start = smax(start_logits)\n    inv_probs_start = 1 - probs_start\n    probs_end = smax(end_logits)\n    inv_probs_end = 1 - probs_end\n    \n    #get log of probabilities. Note: NLLLoss required log probabilities. This is the Natural Log (Log base e)\n    lsmax = nn.LogSoftmax(dim=1)\n    log_probs_start = lsmax(start_logits)\n    log_probs_end = lsmax(end_logits)\n    \n    nll = nn.NLLLoss()\n    \n    fl_start = nll(torch.pow(inv_probs_start, gamma)* log_probs_start, start_positions)\n    fl_end = nll(torch.pow(inv_probs_end, gamma)*log_probs_end, end_positions)\n    \n    #return mean of the Loss for the start and end logits\n    return ((fl_start + fl_end)/2)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:14:13.578650Z","iopub.execute_input":"2023-12-07T11:14:13.579035Z","iopub.status.idle":"2023-12-07T11:14:13.585965Z","shell.execute_reply.started":"2023-12-07T11:14:13.579007Z","shell.execute_reply":"2023-12-07T11:14:13.585027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optim = AdamW(model.parameters(), lr=2e-5, weight_decay=2e-2)\nscheduler = ExponentialLR(optim, gamma=0.9)\ntotal_acc = []\ntotal_loss = []\n","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:14:14.807570Z","iopub.execute_input":"2023-12-07T11:14:14.807953Z","iopub.status.idle":"2023-12-07T11:14:14.819465Z","shell.execute_reply.started":"2023-12-07T11:14:14.807923Z","shell.execute_reply":"2023-12-07T11:14:14.818492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_epoch(model, dataloader, epoch):\n    model = model.train()\n    losses = []\n    acc = []\n    ctr = 0\n    batch_tracker = 0\n    for batch in tqdm(dataloader, desc = 'Running Epoch '):\n        optim.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n        out_start, out_end = model(input_ids=input_ids, \n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids)\n        #loss = loss_fn(out_start, out_end, start_positions, end_positions)  # <---BASELINE.  Cross Entropy Loss is returned by Default\n        loss = focal_loss_fn(out_start, out_end, start_positions, end_positions,1) #using gamma = 1\n        losses.append(loss.item())\n        loss.backward()\n        optim.step()\n        \n        start_pred = torch.argmax(out_start, dim=1)\n        end_pred = torch.argmax(out_end, dim=1)\n            \n        acc.append(((start_pred == start_positions).sum()/len(start_pred)).item())\n        acc.append(((end_pred == end_positions).sum()/len(end_pred)).item())\n        #ctr = ctr +1\n        #if ctr==50:\n        #    break\n        batch_tracker = batch_tracker + 1\n        if batch_tracker==250 and epoch==1:\n            total_acc.append(sum(acc)/len(acc))\n            loss_avg = sum(losses)/len(losses)\n            total_loss.append(loss_avg)\n            batch_tracker = 0\n    scheduler.step()\n    ret_acc = sum(acc)/len(acc)\n    ret_loss = sum(losses)/len(losses)\n    return(ret_acc, ret_loss)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:14:15.787954Z","iopub.execute_input":"2023-12-07T11:14:15.788322Z","iopub.status.idle":"2023-12-07T11:14:15.799712Z","shell.execute_reply.started":"2023-12-07T11:14:15.788291Z","shell.execute_reply":"2023-12-07T11:14:15.798670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, dataloader):\n    model = model.eval()\n    losses = []\n    acc = []\n    ctr = 0\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc = 'Running Evaluation'):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            start_true = batch['start_positions'].to(device)\n            end_true = batch['end_positions'].to(device)\n            \n            out_start, out_end = model(input_ids=input_ids, \n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids)\n            \n            start_pred = torch.argmax(out_start, dim=1)\n            end_pred = torch.argmax(out_end, dim=1)\n            \n            acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n            acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n            #ctr = ctr +1\n            #if ctr==50:\n            #    break\n        ret_acc = sum(acc)/len(acc)\n        ret_loss = 0\n        #ret_loss = sum(losses)/len(losses)\n    return(ret_acc)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:14:16.728074Z","iopub.execute_input":"2023-12-07T11:14:16.728452Z","iopub.status.idle":"2023-12-07T11:14:16.737359Z","shell.execute_reply.started":"2023-12-07T11:14:16.728420Z","shell.execute_reply":"2023-12-07T11:14:16.736371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 5\n\nmodel.to(device)\n\nfor epoch in range(EPOCHS):\n    train_acc, train_loss = train_epoch(model, train_data_loader, epoch+1)\n    print(f\"Train Accuracy: {train_acc}      Train Loss: {train_loss}\")\n    val_acc = eval_model(model, valid_data_loader)\n    print(f\"Validation Accuracy: {val_acc}\")\n\ntorch.save(model.state_dict(), \"QA_finutunemodel.pt\")","metadata":{"execution":{"iopub.status.busy":"2023-12-07T11:14:17.403377Z","iopub.execute_input":"2023-12-07T11:14:17.403780Z","iopub.status.idle":"2023-12-07T11:14:29.121905Z","shell.execute_reply.started":"2023-12-07T11:14:17.403733Z","shell.execute_reply":"2023-12-07T11:14:29.120647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_answer(question, context):\n    inputs = tokenizerFast.encode_plus(question, context, return_tensors='pt').to(device)\n    with torch.no_grad():\n        output_start, output_end = model(**inputs)\n        \n        answer_start = torch.argmax(output_start)  \n        answer_end = torch.argmax(output_end) \n\n        answer = tokenizerFast.convert_tokens_to_string(tokenizerFast.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n\n        return(answer)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T07:06:32.578919Z","iopub.execute_input":"2023-12-07T07:06:32.579766Z","iopub.status.idle":"2023-12-07T07:06:32.587220Z","shell.execute_reply.started":"2023-12-07T07:06:32.579722Z","shell.execute_reply":"2023-12-07T07:06:32.585977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Finding start and end span indices\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}